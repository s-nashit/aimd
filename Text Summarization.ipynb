{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0692827b-c157-4211-9dbf-aee8c2b83d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0afaed00-f889-4c5e-bc36-e53f2ece165a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s_nas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\s_nas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a409071-33e1-4158-b1aa-ff8aafc1c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text):\n",
    "    stop_words =set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    # return words\n",
    "\n",
    "    freq = {}\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word not in stop_words:\n",
    "            freq[word] =freq.get(word, 0)+1\n",
    "        # return word\n",
    "    # return freq\n",
    "\n",
    "    sentences = sent_tokenize(text)\n",
    "    snt_s = {}\n",
    "    for sent in sentences:\n",
    "        for word in word_tokenize(sent.lower()):\n",
    "            if word in freq:\n",
    "                snt_s[sent] = snt_s.get(sent, 0) +freq[word]\n",
    "            # return snt_s\n",
    "\n",
    "    summary_sentences = sorted(snt_s, key =snt_s.get, reverse=True)\n",
    "    summary = \" \".join(summary_sentences)\n",
    "    return summary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ee609bb-988d-41b5-a657-644f834ed8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rank the sentences based on their scores and select the top-ranking sentences for inclusion in the summary.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_text('''Rank the sentences based on their scores and select the top-ranking sentences for inclusion in the summary.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6ba5041-7b3d-4c1b-9936-b52bf204230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter the text Automatic text summarization aims to generate concise summaries of given documents, condensing the main ideas and key information. This is done while maintaining the essence of the original text. There are two main approaches to automatic text summarization: extractive and abstractive. In this project, we will focus on extractive summarization. Key features of building an extractive text summarization model include the following\n"
     ]
    }
   ],
   "source": [
    "user = input('enter the text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fc57934-3558-475a-adec-c701d01f8cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Automatic text summarization aims to generate concise summaries of given documents, condensing the main ideas and key information. There are two main approaches to automatic text summarization: extractive and abstractive. Key features of building an extractive text summarization model include the following In this project, we will focus on extractive summarization. This is done while maintaining the essence of the original text.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_text(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de8abe0e-ff5d-4c6b-a941-1fb767158dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e66c765-16dd-46fe-a9e3-49adf42194cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "477ebe6e-53b8-4908-8cf4-25ea69d14b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''In this article, we shall look at a working example of extractive summarization. Below is the algorithm implemented in the gensim library, called \"TextRank\", which is based on PageRank algorithm for ranking search results. Pre-process the given text. This includes stop'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07e28a6c-562b-4a99-9f95-155b079b10f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this article, we shall look at a working example of extractive summarization.\\nBelow is the algorithm implemented in the gensim library, called \"TextRank\", which is based on PageRank algorithm for ranking search results.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text, ratio = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc34f940-657e-4046-be7a-97d0ae52b9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In this article, we shall look at a working example of extractive summarization. Below is the algorithm implemented in the gensim library, called \"TextRank\", which is based on PageRank algorithm for ranking search results. Pre-process the given text. This includes stop'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4e341d1-04de-4de0-ada3-5ae50dbcb931",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Summarization is a useful tool for varied textual applications that aims to highlight important information within a large corpus. With the outburst of information on the web, Python provides some handy tools to help summarize a text. This article provides an overview of the two major categories of approaches followed - extractive and abstractive. In this article, we shall look at a working example of extractive summarization.\n",
    " \n",
    "\n",
    "Algorithm : \n",
    "Below is the algorithm implemented in the gensim library, called \"TextRank\", which is based on PageRank algorithm for ranking search results.\n",
    "\n",
    "Pre-process the given text. This includes stop words removal, punctuation removal, and stemming.\n",
    "Make a graph with sentences that are the vertices.\n",
    "The graph has edges denoting the similarity between the two sentences at the vertices.\n",
    "Run PageRank algorithm on this weighted graph.\n",
    "Pick the highest-scoring vertices and append them to the summary.\n",
    "Based on the ratio or the word count, the number of vertices to be picked is decided.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b741e87-d5ac-4744-983b-9aeed09b0973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'With the outburst of information on the web, Python provides some handy tools to help summarize a text.\\nRun PageRank algorithm on this weighted graph.\\nBased on the ratio or the word count, the number of vertices to be picked is decided.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(text, ratio=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b32451e1-f1b2-4b04-a396-c719de75f4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summarization is a useful tool for varied textual applications that aims to highlight important information within a large corpus. With the outburst of information on the web, Python provides some handy tools to help summarize a text. This article provides an overview of the two major categories of approaches followed - extractive and abstractive. In this article, we shall look at a working example of extractive summarization.\\n\\n\\nAlgorithm : \\nBelow is the algorithm implemented in the gensim library, called \"TextRank\", which is based on PageRank algorithm for ranking search results.\\n\\nPre-process the given text. This includes stop words removal, punctuation removal, and stemming.\\nMake a graph with sentences that are the vertices.\\nThe graph has edges denoting the similarity between the two sentences at the vertices.\\nRun PageRank algorithm on this weighted graph.\\nPick the highest-scoring vertices and append them to the summary.\\nBased on the ratio or the word count, the number of vertices to be picked is decided.\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0153a152-2fd0-4f90-8d2a-512f211c2a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Word Suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e38e7980-f9f2-43ac-8a65-a4d298c4f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import regex as reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b0eaa01-d68e-451b-8cb2-cc783f0bdfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9f0ab226-489d-441f-aba3-23614328be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Hi, How are you.\n",
    "I’m sorry for my behavior earlier.\n",
    "I apologize for what I said.\n",
    "I shouldn’t have acted that way, I’m sorry.\n",
    "Please forgive me for my mistake.\n",
    "I regret what I did, and I apologize.\n",
    "I apologize if I offended you.\n",
    "I’m sorry for any inconvenience caused.\n",
    "Please accept my sincerest apologies.\n",
    "I’m sorry for being late.\n",
    "I’m sorry for not showing up as planned.\n",
    "Please forgive me for my absence.\n",
    "I’m sorry for my lack of communication.\n",
    "I apologize for my misunderstanding.\n",
    "I’m sorry for not listening to you.\n",
    "I’m sorry for letting you down.\n",
    "Please forgive me for breaking your trust.\n",
    "I’m sorry for the harm I caused.\n",
    "I apologize for my behavior, it was unacceptable.\n",
    "I’m sorry for my mistake, I’ll make it right.\n",
    "I’m sorry for any hurt feelings.\n",
    "I’m sorry for not meeting your expectations.\n",
    "I apologize for not being there for you.\n",
    "I’m sorry for my part in the misunderstanding.\n",
    "I apologize for any confusion I caused.\n",
    "I’m sorry for my insensitive comment.\n",
    "Please forgive me for my thoughtlessness.\n",
    "I’m sorry for being disrespectful.\n",
    "I’m sorry for my part in the argument.\n",
    "I apologize for my lack of consideration.\n",
    "I’m sorry for not being more understanding.\n",
    "I’m sorry for my tone of voice.\n",
    "I apologize for my impatience.\n",
    "I’m sorry for not showing enough appreciation.\n",
    "I’m sorry for my mistake, it won’t happen again.\n",
    "I apologize for not being more aware.\n",
    "Please forgive me for my unkind words.\n",
    "I’m sorry for not being more supportive.\n",
    "Let’s make a plan for the weekend.\n",
    "I need to start making plans for my summer vacation.\n",
    "We should make a plan for the next meeting.\n",
    "Can we make a plan to go hiking next weekend?\n",
    "Let’s make a plan to try a new restaurant this week.\n",
    "I’m going to make a plan to organize my closet this weekend.\n",
    "We need to make a plan for the project deadline.\n",
    "Can we make a plan to go to the beach next month?\n",
    "Let’s make a plan to visit the museum on Saturday.\n",
    "I need to make a plan to study for my exams.\n",
    "We should make a plan to start exercising regularly.\n",
    "Can we make a plan to have a game night with friends?\n",
    "Let’s make a plan to volunteer at the local shelter.\n",
    "I’m going to make a plan to save money for a trip.\n",
    "We need to make a plan for the office party.\n",
    "Can we make a plan to have a picnic in the park.\n",
    "Let’s make a plan to attend a concert next month.\n",
    "I need to make a plan to learn a new skill.\n",
    "We should make a plan to spend more time with family.\n",
    "Can we make a plan to take a cooking class together.\n",
    "Let’s make a plan to visit the botanical gardens next week.\n",
    "I’m going to make a plan to finish reading a book this month.\n",
    "We need to make a plan to clean the house before guests arrive.\n",
    "Can we make a plan to have a movie night at home.\n",
    "Let’s make a plan to try a new workout routine.\n",
    "I need to make a plan to renew my passport.\n",
    "We should make a plan to take a road trip this summer.\n",
    "Can we make a plan to take a day off and go to the spa.\n",
    "Let’s make a plan to go camping next month.\n",
    "I’m going to make a plan to start a garden this spring.\n",
    "We need to make a plan to redecorate the living room.\n",
    "Can we make a plan to have a dinner party with friends.\n",
    "Let’s make a plan to go to a comedy show this weekend.\n",
    "I need to make a plan to finish a project at work.\n",
    "We should make a plan to go skiing next winter.\n",
    "Can we make a plan to have a book club with coworkers.\n",
    "Let’s make a plan to take a dance class together.\n",
    "I’m going to make a plan to visit a new city this year.\n",
    "We need to make a plan to donate to a charity.\n",
    "Can we make a plan to have a photo shoot with friends?\n",
    "Let’s make a plan to attend a sports game next week.\n",
    "I need to make a plan to learn a new language.\n",
    "We should make a plan to have a potluck with neighbors.\n",
    "Can we make a plan to take a yoga class together?\n",
    "Let’s make a plan to go on a bike ride this weekend.\n",
    "I’m going to make a plan to try a new hobby.\n",
    "We need to make a plan to plan a surprise party.\n",
    "Can we make a plan to have a wine tasting with friends.\n",
    "Let’s make a plan to attend a festival this summer.\n",
    "I need to make a plan to relax and take a day off.\n",
    "I’m sorry for not acknowledging your feelings.\n",
    "I apologize for my part in the misunderstanding.\n",
    "I’m sorry for not taking responsibility.\n",
    "I apologize for not being more accountable.\n",
    "I’m sorry for my thoughtless actions.\n",
    "I’m sorry for not being more responsible.\n",
    "I’m sorry for my lack of empathy.\n",
    "Please forgive me for my forgetfulness.\n",
    "I’m sorry for my lack of consideration.\n",
    "I apologize for my lack of attention.\n",
    "I’m sorry for not being more reliable.\n",
    "I’m sorry for not being more present.\n",
    "Please accept my apologies, I am truly sorry.\n",
    "Where are you from.\n",
    "How’s your day going so far?\n",
    "Thank you so much for your help!\n",
    "I really appreciate your support.\n",
    "Your kindness means a lot to me.\n",
    "I am grateful for your generosity.\n",
    "I cannot thank you enough for everything you have done for me.\n",
    "You have my sincere thanks for your assistance.\n",
    "Your contribution has made a significant difference.\n",
    "Your thoughtfulness is truly appreciated.\n",
    "Your help was invaluable.\n",
    "I am deeply grateful for your understanding.\n",
    "I feel so fortunate to have you in my life.\n",
    "I cannot express my gratitude enough.\n",
    "Your support has been a blessing.\n",
    "Thank you for being there for me.\n",
    "Your encouragement has meant a lot to me.\n",
    "I am grateful for your trust in me.\n",
    "Your compassion has made a difference.\n",
    "I am thankful for your friendship.\n",
    "Your positive attitude is inspiring.\n",
    "I am grateful for the time and effort you have put into this.\n",
    "Your generosity is overwhelming.\n",
    "I am thankful for your patience.\n",
    "Your guidance has been invaluable.\n",
    "Your support has been a source of strength.\n",
    "I appreciate your loyalty.\n",
    "Your help has been a lifesaver.\n",
    "I am grateful for your wisdom.\n",
    "Your kindness has touched my heart.\n",
    "Your contribution is greatly appreciated.\n",
    "Thank you for going above and beyond.\n",
    "Your assistance has been a great help.\n",
    "I am grateful for your faith in me.\n",
    "Your generosity has made a big difference.\n",
    "Your kindness is a gift I will always cherish.\n",
    "I am thankful for your selflessness.\n",
    "Your support has been unwavering.\n",
    "Your encouragement has been a source of motivation.\n",
    "I am grateful for your thoughtfulness.\n",
    "Your help has been instrumental in my success.\n",
    "I appreciate your understanding and support.\n",
    "Your generosity is truly inspiring.\n",
    "I am grateful for your kindness and compassion.\n",
    "Your guidance has been invaluable to me.\n",
    "Thank you for being a wonderful friend.\n",
    "Your support has been a ray of light in difficult times.\n",
    "I am grateful for your unwavering support.\n",
    "Your encouragement has helped me achieve my goals.\n",
    "I am thankful for your generosity and kindness.\n",
    "Your help has made a significant impact.\n",
    "I am grateful for your presence in my life.\n",
    "What have you been up to lately?\n",
    "Did you catch the game last night?\n",
    "How’s the weather treating you?\n",
    "Have you tried any good restaurants in the area recently?\n",
    "How was your weekend?\n",
    "Are you planning any trips or vacations soon?\n",
    "What do you like to do for fun?\n",
    "Do you have any pets?\n",
    "How’s your family doing?\n",
    "What do you think of this place?\n",
    "Have you seen any good movies or TV shows lately?\n",
    "How’s work been treating you?\n",
    "Have you read any good books lately?\n",
    "What’s your favorite type of music?\n",
    "Have you tried any new hobbies recently?\n",
    "Do you have any exciting plans for the future?\n",
    "What’s your favorite type of cuisine?\n",
    "How do you like to spend your free time?\n",
    "What’s your favorite sports team?\n",
    "Go straight ahead for about two blocks.\n",
    "Turn left/right at the intersection.\n",
    "Walk/drive towards the park/restaurant/mall.\n",
    "Keep going until you see the big red building.\n",
    "Cross the street and continue walking/driving.\n",
    "Take the second street on your left/right.\n",
    "Follow the signs to the airport/train station/bus stop.\n",
    "Stay on this road until you reach the traffic lights.\n",
    "Make a U-turn at the next roundabout.\n",
    "Take the exit for the city center/your destination.\n",
    "Use the pedestrian crossing to cross the busy road.\n",
    "Look out for the statue/fountain/sign on your right/left.\n",
    "Walk past the supermarket and turn right at the corner.\n",
    "Go down the stairs/escalator to the subway platform.\n",
    "Take the elevator to the 3rd floor and turn left.\n",
    "Walk through the park and follow the path to the lake.\n",
    "Head towards the tall office building in front of you.\n",
    "Go through the archway and turn right at the end.\n",
    "Keep walking until you see the entrance to the museum.\n",
    "Turn left at the traffic lights and the hotel is on your right.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "510c0a3c-6191-4166-b77b-28008b2e5bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "067b150e-e2b2-42b2-b03d-1df8afa48ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words =len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2bddc6b7-cf09-4732-8164-2a8fe0650a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "412"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eaf82677-325e-439f-9993-0d888d445fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e54467e0-0b29-40e6-961d-c4f64e75be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in text.split('.'):\n",
    "    token_list = t.texts_to_sequences([sentence])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        input_sequences.append(token_list[:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "972c3734-2398-4058-a723-67324131dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max([len(seq) for seq in input_sequences])\n",
    "inp_seq = pad_sequences(input_sequences, maxlen = max_len, padding ='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0cf56ca6-1c36-4532-9a6d-c71382bfebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = inp_seq[:, :-1], inp_seq[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "424d04bf-9bcf-4430-b79a-ec6723621e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   0, 156],\n",
       "       [  0,   0,   0, ...,   0, 156,  61],\n",
       "       [  0,   0,   0, ..., 156,  61,  62],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   5, 411,  42],\n",
       "       [  0,   0,   0, ..., 411,  42,  46],\n",
       "       [  0,   0,   0, ...,  42,  46,   4]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e59cf696-908a-4db8-b793-a5bb9e36f55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1403"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "51846e97-5db4-46a2-8b59-6f7300efbd39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1403, 412)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c00e21b5-a22e-440f-a06e-a4a2364bd457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1403, 135)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4da47325-33ff-444a-b3c6-5824b2d3f20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[156, 61]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7315a183-3f82-4ab2-8c6d-f8a0196dca99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0, 156,  61],\n",
       "       [  0,   0,   0, ..., 156,  61,  62],\n",
       "       [  0,   0,   0, ...,  61,  62,  11],\n",
       "       ...,\n",
       "       [  0,   0,   0, ..., 411,  42,  46],\n",
       "       [  0,   0,   0, ...,  42,  46,   4],\n",
       "       [  0,   0,   0, ...,  46,   4,  38]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5397c600-15a0-4fc7-baa7-02923ba9f2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "76e1456d-93e2-409b-8c57-8a87b0684e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3d45745d-7ed3-40cb-9983-d50371b83f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\s_nas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding(total_words, 10, input_length = max_len-1),\n",
    "    LSTM(100),\n",
    "    Dense(total_words, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e12f62d4-c3d3-47fc-ad99-9bf4599d9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "58d0de31-ae1a-40b6-b3eb-0b6457ffa122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - accuracy: 0.3990 - loss: 2.6242\n",
      "Epoch 2/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.4377 - loss: 2.5134\n",
      "Epoch 3/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.4405 - loss: 2.4625\n",
      "Epoch 4/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.4480 - loss: 2.4321\n",
      "Epoch 5/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.4686 - loss: 2.3572\n",
      "Epoch 6/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.4809 - loss: 2.2490\n",
      "Epoch 7/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.4593 - loss: 2.2932\n",
      "Epoch 8/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.4755 - loss: 2.1995\n",
      "Epoch 9/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.4793 - loss: 2.1793\n",
      "Epoch 10/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.4939 - loss: 2.1287\n",
      "Epoch 11/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.5136 - loss: 2.0668\n",
      "Epoch 12/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.5138 - loss: 2.0490\n",
      "Epoch 13/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.5060 - loss: 2.0174\n",
      "Epoch 14/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.5402 - loss: 1.9510\n",
      "Epoch 15/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.5252 - loss: 1.9053\n",
      "Epoch 16/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.5501 - loss: 1.8577\n",
      "Epoch 17/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.5387 - loss: 1.8312\n",
      "Epoch 18/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.5380 - loss: 1.8462\n",
      "Epoch 19/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.5918 - loss: 1.6983\n",
      "Epoch 20/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.5841 - loss: 1.6894\n",
      "Epoch 21/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.5842 - loss: 1.6831\n",
      "Epoch 22/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.6020 - loss: 1.5828\n",
      "Epoch 23/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.5970 - loss: 1.6291\n",
      "Epoch 24/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.6330 - loss: 1.4908\n",
      "Epoch 25/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.6251 - loss: 1.5484\n",
      "Epoch 26/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.6295 - loss: 1.4741\n",
      "Epoch 27/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.6436 - loss: 1.4471\n",
      "Epoch 28/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.6169 - loss: 1.4729\n",
      "Epoch 29/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.6388 - loss: 1.4253\n",
      "Epoch 30/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.6297 - loss: 1.4674\n",
      "Epoch 31/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.6604 - loss: 1.3948\n",
      "Epoch 32/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.6808 - loss: 1.3465\n",
      "Epoch 33/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.6916 - loss: 1.2963\n",
      "Epoch 34/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.6761 - loss: 1.3301\n",
      "Epoch 35/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.7038 - loss: 1.2478\n",
      "Epoch 36/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.7283 - loss: 1.1959\n",
      "Epoch 37/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.7134 - loss: 1.2407\n",
      "Epoch 38/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.7184 - loss: 1.2094\n",
      "Epoch 39/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.7161 - loss: 1.2281\n",
      "Epoch 40/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.7366 - loss: 1.1138\n",
      "Epoch 41/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.7344 - loss: 1.0986\n",
      "Epoch 42/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.7531 - loss: 1.0907\n",
      "Epoch 43/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.7332 - loss: 1.0878\n",
      "Epoch 44/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.7312 - loss: 1.0656\n",
      "Epoch 45/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.7652 - loss: 0.9864\n",
      "Epoch 46/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.7563 - loss: 1.0355\n",
      "Epoch 47/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.7623 - loss: 0.9850\n",
      "Epoch 48/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.7652 - loss: 0.9593\n",
      "Epoch 49/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.7633 - loss: 0.9605\n",
      "Epoch 50/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.7572 - loss: 0.9326\n",
      "Epoch 51/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.7701 - loss: 0.9442\n",
      "Epoch 52/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.7692 - loss: 0.9382\n",
      "Epoch 53/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.7700 - loss: 0.9106\n",
      "Epoch 54/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.7743 - loss: 0.8568\n",
      "Epoch 55/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.7828 - loss: 0.8713\n",
      "Epoch 56/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.7976 - loss: 0.8595\n",
      "Epoch 57/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.7728 - loss: 0.8592\n",
      "Epoch 58/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - accuracy: 0.7778 - loss: 0.8324\n",
      "Epoch 59/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.8143 - loss: 0.7888\n",
      "Epoch 60/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.7918 - loss: 0.7960\n",
      "Epoch 61/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.7841 - loss: 0.8163\n",
      "Epoch 62/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step - accuracy: 0.7973 - loss: 0.7907\n",
      "Epoch 63/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.8004 - loss: 0.7597\n",
      "Epoch 64/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.8045 - loss: 0.7436\n",
      "Epoch 65/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.7923 - loss: 0.7719\n",
      "Epoch 66/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.8005 - loss: 0.7389\n",
      "Epoch 67/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.8116 - loss: 0.7051\n",
      "Epoch 68/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.8171 - loss: 0.7060\n",
      "Epoch 69/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.8042 - loss: 0.7283\n",
      "Epoch 70/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.8042 - loss: 0.6774\n",
      "Epoch 71/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.8289 - loss: 0.6719\n",
      "Epoch 72/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.7996 - loss: 0.6939\n",
      "Epoch 73/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.8285 - loss: 0.6501\n",
      "Epoch 74/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step - accuracy: 0.8133 - loss: 0.6891\n",
      "Epoch 75/75\n",
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 75ms/step - accuracy: 0.8181 - loss: 0.6301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1fdb49daea0>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x,y, epochs = 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7e57bc90-e0b4-4c4a-a82d-3da5519b72f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(seed):\n",
    "    token_list = t.texts_to_sequences([seed])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen = max_len-1, padding= 'pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    return t.index_word[predicted[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2b90b190-5986-40e5-9510-53fb48c005d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'am'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('I')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1708acd4-63b3-43b0-8dcd-f2ace2a642ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
